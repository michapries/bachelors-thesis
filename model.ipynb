{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing\n",
    "2. Data Extraction\n",
    "3. Data Exploration\n",
    "4. **Model**\n",
    "\n",
    "This file initializes the model and makes predictions too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature dataframe from pickle file\n",
    "df = pd.read_pickle('feature_frame.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeID</th>\n",
       "      <th>AwayID</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>Date</th>\n",
       "      <th>H_WIN_PCT_home</th>\n",
       "      <th>H_DRAW_PCT_home</th>\n",
       "      <th>A_WIN_PCT_home</th>\n",
       "      <th>A_DRAW_PCT_home</th>\n",
       "      <th>...</th>\n",
       "      <th>REL_PTS_1_away</th>\n",
       "      <th>REL_PTS_2_away</th>\n",
       "      <th>REL_PTS_3_away</th>\n",
       "      <th>REL_PTS_4_away</th>\n",
       "      <th>REL_PTS_5_away</th>\n",
       "      <th>REL_PTS_N-0_away</th>\n",
       "      <th>REL_PTS_N-1_away</th>\n",
       "      <th>REL_PTS_N-2_away</th>\n",
       "      <th>REL_PTS_N-3_away</th>\n",
       "      <th>REL_PTS_N-4_away</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>107</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-19</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.696970</td>\n",
       "      <td>-0.515152</td>\n",
       "      <td>-0.424242</td>\n",
       "      <td>-0.424242</td>\n",
       "      <td>-0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>61</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-09-02</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-10-23</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12357</th>\n",
       "      <td>119</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-05</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>...</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-11-24</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.384615</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>-1.230769</td>\n",
       "      <td>-1.181818</td>\n",
       "      <td>-1.083333</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HomeID  AwayID  FTHG  FTAG  FTR        Date  H_WIN_PCT_home  \\\n",
       "11984     107      98     0     0    0  2016-11-19        0.815789   \n",
       "5884       61      56     1     1    0  2018-09-02        0.617647   \n",
       "84         14      16     2     1    1  2010-10-23        0.500000   \n",
       "12357     119     106     3     1    1  2017-11-05        0.447368   \n",
       "883         4      14     2     4    2  2012-11-24        0.342857   \n",
       "\n",
       "       H_DRAW_PCT_home  A_WIN_PCT_home  A_DRAW_PCT_home  ...  REL_PTS_1_away  \\\n",
       "11984         0.078947        0.736842         0.131579  ...        1.166667   \n",
       "5884          0.176471        0.411765         0.235294  ...        0.000000   \n",
       "84            0.500000        0.250000         0.250000  ...        1.444444   \n",
       "12357         0.210526        0.263158         0.157895  ...        1.818182   \n",
       "883           0.285714        0.236842         0.289474  ...        0.333333   \n",
       "\n",
       "       REL_PTS_2_away  REL_PTS_3_away  REL_PTS_4_away  REL_PTS_5_away  \\\n",
       "11984        0.833333        0.666667        0.575758        0.416667   \n",
       "5884         0.000000        0.000000        0.000000        0.000000   \n",
       "84           1.125000        0.750000        0.750000        0.666667   \n",
       "12357        1.454545        1.090909        1.000000        0.727273   \n",
       "883          0.307692        0.000000        0.000000       -0.384615   \n",
       "\n",
       "       REL_PTS_N-0_away  REL_PTS_N-1_away  REL_PTS_N-2_away  REL_PTS_N-3_away  \\\n",
       "11984         -0.696970         -0.515152         -0.424242         -0.424242   \n",
       "5884           0.000000          0.000000          0.000000          0.000000   \n",
       "84            -0.250000         -0.250000          0.000000          0.000000   \n",
       "12357         -0.454545         -0.400000         -0.200000          0.000000   \n",
       "883           -1.333333         -1.230769         -1.181818         -1.083333   \n",
       "\n",
       "       REL_PTS_N-4_away  \n",
       "11984         -0.424242  \n",
       "5884           0.000000  \n",
       "84             0.111111  \n",
       "12357          0.000000  \n",
       "883           -1.000000  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting target variables and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _WICHTIG: Das hier muss auf neue columns angepasst werden (z.B. season)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, the goal difference for a specific game is seen as the target variable.\n",
    "# (e.g. -3 for a game outcome of 1:4, or 2 for 3:1) .\n",
    "\n",
    "# If False, we just want to predict the winner.\n",
    "# 1 = Home team wins, 0 = Draw, 2 = Away team wins\n",
    "predict_goal_difference = False\n",
    "\n",
    "if predict_goal_difference:\n",
    "    y = df['FTHG'] - df['FTAG']\n",
    "else:\n",
    "    y = df['FTR']\n",
    "    \n",
    "# Remove unnecessary columns (IDs etc.) from features\n",
    "X = df.iloc[:,6:].drop(['season', 'GAME_CNT_AFTER_GAME_home', 'GAME_CNT_AFTER_GAME_away', 'league'], axis=1)    \n",
    "#X['FTR'] = df['FTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,6:].drop(['season', 'GAME_CNT_AFTER_GAME_home', 'GAME_CNT_AFTER_GAME_away'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Probability Score (RPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rps(pred, actual_value, r=3):\n",
    "    '''Returns the ranked probability score for a single given game.\n",
    "    (see Hubacek paper for formula)\n",
    "    \n",
    "    Arguments:\n",
    "    pred -- predicted results; in vector form (e.g. [0.1, 0.6, 0.3])\n",
    "    actual_value -- actual result (0, 1 or 2); not in vector form yet\n",
    "    r -- number of categories (3 for football)\n",
    "    '''\n",
    "    value_vec = [0, 0, 0]\n",
    "    \n",
    "    # Bring value_vec into 1, 0, 2 order\n",
    "    if actual_value == 0:\n",
    "        value_vec[1] = 1\n",
    "    elif actual_value == 2:\n",
    "        value_vec[2] = 1\n",
    "    elif actual_value == 1:\n",
    "        value_vec[0] = 1\n",
    "    else:\n",
    "        print(actual_value)\n",
    "        raise Exception('Prediction was not in [1, 0, 2].')\n",
    "    #value_vec = [0, 0, 1]\n",
    "    \n",
    "    #print(pred)\n",
    "    #print(\"pred:\", pred, \"vec:\", value_vec, \"actualval:\", actual_value)\n",
    "    pred[0], pred[1], pred[2] = pred[1], pred[0], pred[2]   # order: loss, draw, win\n",
    "    #print(\"pred:\", pred, \"vec:\", value_vec, \"actualval:\", actual_value)\n",
    "    \n",
    "    rps = 0\n",
    "    \n",
    "    for i in range(0, r-1):\n",
    "        inner_sum = 0\n",
    "        for j in range(0, i+1):\n",
    "            inner_sum += (pred[j] - value_vec[j])\n",
    "        rps += np.square(inner_sum)\n",
    "    \n",
    "    rps /= (r-1)\n",
    "    \n",
    "    return rps\n",
    "\n",
    "\n",
    "# To be used as eval_metric parameter\n",
    "def rps_eval_metric(y_true, y_pred):\n",
    "    return rps(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently the random states are pretty important. 21 works very well on RPS and test accuracy, 16 only on accuracy.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=543)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best value for *n_estimators*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We start with n_estimators=50...\n",
    "xgb_cl = xgb.XGBClassifier(objective='multi:softprob', n_estimators=50, seed=16)\n",
    "# ...and then validate it to find the lowest loss.\n",
    "op = xgb_cl.fit(X_train, y_train, early_stopping_rounds=900, eval_metric='mlogloss', eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.2, 0.3, 0.4]\n",
    "n_estimators = [8, 16, 32]\n",
    "subsamples = [0.6, 0.8, 0.9, 1]\n",
    "colsample_bytree = [0.5, 0.75, 0.9, 1]\n",
    "max_depth = [3, 6, 8, 12]\n",
    "param_grid = dict(n_estimators=n_estimators, learning_rate=learning_rates, subsample=subsamples, colsample_bytree=colsample_bytree, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python38\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:56:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best: 0.604363 using {'colsample_bytree': 0.75, 'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 16, 'subsample': 1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=xgb_cl, param_grid=param_grid, n_jobs=8, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use best parameters to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators=8 had the lowest loss, so we overwrite the previous model.b\n",
    "#xgb_cl = xgb.XGBClassifier(objective='multi:softprob', n_estimators=8, seed=16, learning_rate=0.3)\n",
    "xgb_cl = xgb.XGBClassifier(objective='multi:softprob', n_estimators=64, seed=16, learning_rate=0.2, subsample=1, colsample_bytree=0.6, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python38\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:21:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.2, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=64, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=16, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, seed=16, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6285400658616904\n"
     ]
    }
   ],
   "source": [
    "preds = xgb_cl.predict(X_test)\n",
    "accuracy = float(np.sum(preds == y_test))/y_test.shape[0]\n",
    "\n",
    "print(f'accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set: 0.7332387371408301\n"
     ]
    }
   ],
   "source": [
    "# Looking at train set accuracy to get an intuition of how much the model overfits\n",
    "preds = xgb_cl.predict(X_train)\n",
    "accuracy = float(np.sum(preds == y_train))/y_train.shape[0]\n",
    "\n",
    "print(f'accuracy on train set: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15589529706195693"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions in form of [0.2, 0.5, 0.3] in the order of draw, home win, away win.\n",
    "# This is because it gets ordered like 0, 1, 2.\n",
    "proba_preds = xgb_cl.predict_proba(X_test)\n",
    "\n",
    "# List of RPS scores for every game in the test set.\n",
    "# Important to use iloc for y_test, otherwise indices would be wrong\n",
    "rps_list = [rps(pred, y_test.iloc[i]) for i, pred in enumerate(proba_preds)]\n",
    "\n",
    "#proba_preds\n",
    "\n",
    "#rps(proba_preds[0], y_test.iloc[0])\n",
    "\n",
    "# Average ranked probability score.\n",
    "np.mean(rps_list)\n",
    "#for i, pred in enumerate(proba_preds):\n",
    "#    print(y_test.iloc[i], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = list()\n",
    "for i in range(1000):\n",
    "    if i < 460:\n",
    "        sample_list.append(1)\n",
    "    elif i < 753:\n",
    "        sample_list.append(2)\n",
    "    else:\n",
    "        sample_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22622734325027444"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# league priors -> rps (DO NOT RUN AGAIN)\n",
    "r_list = []\n",
    "for i in range(len(X_test.index)):\n",
    "    league = X_test.iloc[i]['league']\n",
    "    prob_vector = []\n",
    "    if league == 'D1':\n",
    "        prob_vector = [0.2422, 0.4582, 0.2996]\n",
    "    elif league == 'E1':\n",
    "        prob_vector = [0.2468, 0.4576, 0.2956]\n",
    "    elif league == 'I1':\n",
    "        prob_vector = [0.2575, 0.4538, 0.2888]\n",
    "    elif league == 'SP1':\n",
    "        prob_vector = [0.2371, 0.4781, 0.2848]\n",
    "    \n",
    "    r_list.append(rps(prob_vector, y_test.iloc[i]))\n",
    "    \n",
    "np.mean(r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22696134220988437"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global priors -> rps\n",
    "r_list = []\n",
    "for i in range(len(y_test.index)):\n",
    "    r_list.append(rps([0.2921845, 0.46192225, 0.24589325], y_test.iloc[i]))\n",
    "np.mean(r_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
